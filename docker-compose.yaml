x-airflow-common: &airflow-common
  image: local-cluster-airflow:1.0.0
  environment: &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'

    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
    
    # configure as chaves abaixo
    AIRFLOW__CORE__FERNET_KEY: 'airflow'
    AIRFLOW__WEBSERVER__SECRET_KEY: 'airflow'

    AIRFLOW_UID: 50000

  volumes: &airflow-common-volumes
    - ./dags:/opt/airflow/dags
    - ./logs:/opt/airflow/logs
    - ./plugins:/opt/airflow/plugins

  user: "${AIRFLOW_UID:-50000}:0"

  depends_on: &airflow-common-depends-on
    postgres:
      condition: service_healthy

services:

  spark:
    image: bitnami/spark:latest
    ports:
      - 4040:4040
    volumes:
      - ./spark-app:/app
    environment:
      - SPARK_MODE=master

  spark-master:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"

  spark-worker-1:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077

  spark-worker-2:
    image: bitnami/spark:latest
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=2g
      - SPARK_MASTER_URL=spark://spark-master:7077

  pyspark:
    build: .
    ports:
      - "8888:8888"
    volumes:
      - ./notebooks:/notebooks
    working_dir: /notebooks
    command: ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
    environment:
      - SPARK_VERSION=3.1.2
      - HADOOP_VERSION=3.2
      - SPARK_HOME=/usr/local/spark
      - PATH=/usr/local/spark/bin:$PATH
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2

  postgres:
    image: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    healthcheck:
      test: [ "CMD", "pg_isready", "-p", "5432", "-U", "airflow" ]
      interval: 5s
      retries: 5
    
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 11000:8080
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully
  
  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_MIGRATE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    command:
      - -c
      - |
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    user: "0:0"
    volumes:
      - .:/sources